from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
import os
import threading
import time
import hashlib
import json
from datetime import datetime, timedelta

app = Flask(__name__)
CORS(app)  # React Native'den gelen istekler iÃ§in

# SaÄŸlÄ±k KontrolÃ¼ Endpoint'i
@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        "status": "OK", 
        "service": "AI Chat Service",
        "groq_api": "Connected" if groq_client else "Disconnected"
    }), 200

# ============== YAPILANDIRMA ==============
REQUEST_TIMEOUT = 45  # saniye - istek zaman aÅŸÄ±mÄ± (60'tan 45'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)
MAX_NEW_TOKENS = 300  # chat iÃ§in maksimum token (400'den 300'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - daha hÄ±zlÄ±)
STREAM_RESPONSE = True  # Streaming response aktif
CACHE_ENABLED = True  # Cache aktif
CACHE_TTL = 300  # Cache sÃ¼resi (5 dakika)

# ============== GROQ API (Ãœcretsiz - Llama 3.3 70B) ==============
GROQ_API_KEY = os.environ.get('GROQ_API_KEY', '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')

groq_client = None
try:
    from groq import Groq
    # Connection pooling iÃ§in timeout ve max_retries ayarlarÄ±
    groq_client = Groq(
        api_key=GROQ_API_KEY,
        timeout=30.0,  # API timeout (saniye)
        max_retries=2,  # Retry sayÄ±sÄ±
    )
    print("âœ… Groq API baÄŸlantÄ±sÄ± hazÄ±r (Llama 3.3 70B)")
except ImportError:
    print("âš ï¸ Groq paketi yÃ¼klÃ¼ deÄŸil. 'pip install groq' Ã§alÄ±ÅŸtÄ±rÄ±n.")
except Exception as e:
    print(f"âš ï¸ Groq API hatasÄ±: {e}")

# ============== CACHE MEKANÄ°ZMASI ==============
response_cache = {}
cache_lock = threading.Lock()

def get_cache_key(question: str, history: list = None) -> str:
    """Soru ve geÃ§miÅŸ iÃ§in cache key oluÅŸtur"""
    cache_data = {
        "question": question.lower().strip(),
        "history": [(msg.get('text', ''), msg.get('isUser', False)) for msg in (history or [])[-3:]]  # Son 3 mesaj
    }
    cache_str = json.dumps(cache_data, sort_keys=True)
    return hashlib.md5(cache_str.encode()).hexdigest()

def get_cached_response(cache_key: str):
    """Cache'den response al"""
    if not CACHE_ENABLED:
        return None
    
    with cache_lock:
        if cache_key in response_cache:
            cached_data, cached_time = response_cache[cache_key]
            if datetime.now() - cached_time < timedelta(seconds=CACHE_TTL):
                return cached_data
            else:
                # Expired cache'i temizle
                del response_cache[cache_key]
    return None

def set_cached_response(cache_key: str, response_data: dict):
    """Response'u cache'e kaydet"""
    if not CACHE_ENABLED:
        return
    
    with cache_lock:
        # Cache boyutunu sÄ±nÄ±rla (max 100 entry)
        if len(response_cache) > 100:
            # En eski entry'leri sil
            sorted_cache = sorted(response_cache.items(), key=lambda x: x[1][1])
            for key, _ in sorted_cache[:20]:
                del response_cache[key]
        
        response_cache[cache_key] = (response_data, datetime.now())

# ============== SYSTEM PROMPT ==============
SYSTEM_PROMPT = """Sen Hisse Chat uygulamasÄ±nÄ±n AI finansal asistanÄ±sÄ±n.

ğŸ¯ GÃ¶revlerin:
- Borsa, hisse senedi, kripto para sorularÄ±nÄ± yanÄ±tla
- Teknik ve temel analiz kavramlarÄ±nÄ± aÃ§Ä±kla
- YatÄ±rÄ±m stratejileri hakkÄ±nda bilgi ver
- Piyasa terimleri ve kavramlarÄ± Ã¶ÄŸret

ğŸ“ YanÄ±t KurallarÄ±:
- TÃ¼rkÃ§e yanÄ±t ver
- KÄ±sa ve Ã¶z ol (maksimum 150 kelime)
- Bullet point (â€¢) ve emoji kullan (ğŸ“ˆ ğŸ“Š ğŸ’° ğŸ’¹)
- Ã–nemli bilgileri vurgula
- Her yanÄ±tÄ±n sonuna ekle: "âš ï¸ Bu bilgi yatÄ±rÄ±m tavsiyesi deÄŸildir."

ğŸ“Š Format:
- Liste formatÄ±nÄ± tercih et
- SayÄ±sal verileri belirt
- KarÅŸÄ±laÅŸtÄ±rmalÄ± bilgi ver

Ã–rnek yanÄ±t formatÄ±:
ğŸ“Š BIST 100 Durumu:
â€¢ Mevcut seviye: 9,850 puan
â€¢ GÃ¼nlÃ¼k deÄŸiÅŸim: +1.2%
â€¢ Hacim: 45 milyar TL

ğŸ’¡ Ã–nemli Noktalar:
â€¢ BankacÄ±lÄ±k sektÃ¶rÃ¼ Ã¶ncÃ¼
â€¢ DÃ¶viz etkisi pozitif

âš ï¸ Bu bilgi yatÄ±rÄ±m tavsiyesi deÄŸildir."""

# ============== TIMEOUT YÃ–NETÄ°MÄ° ==============
executor = ThreadPoolExecutor(max_workers=4)

def run_with_timeout(func, timeout, *args, **kwargs):
    """Fonksiyonu timeout ile Ã§alÄ±ÅŸtÄ±r"""
    future = executor.submit(func, *args, **kwargs)
    try:
        return future.result(timeout=timeout)
    except FuturesTimeoutError:
        return None

# ============== STREAMING RESPONSE HELPER ==============
def generate_streaming_response(stream, question: str):
    """Streaming response generator"""
    full_text = ""
    actions_generated = False
    
    try:
        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content:
                    content = delta.content
                    full_text += content
                    # Her chunk'Ä± JSON olarak gÃ¶nder
                    yield f"data: {json.dumps({'chunk': content, 'partial': True})}\n\n"
        
        # Stream tamamlandÄ±ÄŸÄ±nda final response gÃ¶nder
        actions = generate_actions(question)
        final_response = {
            'chunk': '',
            'partial': False,
            'full_text': full_text,
            'actions': actions,
            'model': 'llama-3.3-70b',
            'done': True
        }
        yield f"data: {json.dumps(final_response)}\n\n"
    except Exception as e:
        error_response = {
            'error': str(e),
            'partial': False,
            'done': True
        }
        yield f"data: {json.dumps(error_response)}\n\n"

# ============== AKSÄ°YON BUTON OLUÅTURUCU ==============
def generate_actions(question: str):
    """Soruya gÃ¶re aksiyon butonlarÄ± oluÅŸtur"""
    actions = []
    q = question.lower()
    
    # Hisse/Borsa sorularÄ±
    if any(w in q for w in ["hisse", "bist", "borsa", "endeks", "thyao", "sise", "garan", "akbnk"]):
        actions.append({"label": "Grafik GÃ¶r", "icon": "ğŸ“Š", "action": "showChart"})
    
    # Analiz sorularÄ±
    if any(w in q for w in ["analiz", "teknik", "temel", "deÄŸerleme", "rsi", "macd"]):
        actions.append({"label": "DetaylÄ± Analiz", "icon": "ğŸ“ˆ", "action": "detailedAnalysis"})
    
    # KarÅŸÄ±laÅŸtÄ±rma
    if any(w in q for w in ["karÅŸÄ±laÅŸtÄ±r", "vs", "fark", "hangisi", "mÄ± yoksa"]):
        actions.append({"label": "KarÅŸÄ±laÅŸtÄ±r", "icon": "âš–ï¸", "action": "compareStocks"})
    
    # Kripto
    if any(w in q for w in ["bitcoin", "btc", "ethereum", "eth", "kripto", "coin", "altcoin"]):
        actions.append({"label": "Kripto FiyatlarÄ±", "icon": "â‚¿", "action": "cryptoPrices"})
    
    # DÃ¶viz
    if any(w in q for w in ["dolar", "euro", "dÃ¶viz", "kur", "usd", "eur", "tl"]):
        actions.append({"label": "DÃ¶viz KurlarÄ±", "icon": "ğŸ’±", "action": "exchangeRates"})
    
    return actions

# ============== GROQ CHAT FONKSÄ°YONU ==============
def groq_chat(message: str, history: list = None, stream: bool = False):
    """Groq API ile Llama 3.3 70B chat - Optimize edilmiÅŸ"""
    if not groq_client:
        raise Exception("Groq API baÄŸlantÄ±sÄ± yok")
    
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # GeÃ§miÅŸ mesajlarÄ± ekle (son 4'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - daha hÄ±zlÄ±)
    if history:
        for msg in history[-4:]:  # 6'dan 4'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
            role = "user" if msg.get('isUser') else "assistant"
            messages.append({"role": role, "content": msg.get('text', '')})
    
    messages.append({"role": "user", "content": message})
    
    # Optimize edilmiÅŸ parametreler
    params = {
        "model": "llama-3.3-70b-versatile",
        "messages": messages,
        "max_tokens": MAX_NEW_TOKENS,
        "temperature": 0.6,  # 0.7'den 0.6'ya dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ (daha hÄ±zlÄ±, daha tutarlÄ±)
        "top_p": 0.85,  # 0.9'dan 0.85'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ (daha hÄ±zlÄ±)
        "stream": stream,  # Streaming desteÄŸi
    }
    
    if stream:
        # Streaming response
        stream_response = groq_client.chat.completions.create(**params)
        return stream_response
    else:
        # Normal response
        response = groq_client.chat.completions.create(**params)
        return {
            "text": response.choices[0].message.content,
            "model": "llama-3.3-70b",
            "tokens": response.usage.total_tokens if response.usage else None
        }

# ============== ENDPOINTS ==============

@app.route('/health', methods=['GET'])
def health():
    """SaÄŸlÄ±k kontrolÃ¼ endpoint'i"""
    return jsonify({
        "status": "healthy",
        "groq_connected": groq_client is not None,
        "model": "llama-3.3-70b-versatile",
        "version": "2.1.0",
        "optimizations": {
            "streaming": STREAM_RESPONSE,
            "cache_enabled": CACHE_ENABLED,
            "cache_size": len(response_cache),
            "max_tokens": MAX_NEW_TOKENS,
            "timeout": REQUEST_TIMEOUT
        }
    })

@app.route('/chat', methods=['POST'])
def chat():
    """
    AI Chat Endpoint - Groq Llama 3.3 70B - Optimize edilmiÅŸ
    
    Request:
    {
        "question": "BIST 100 hakkÄ±nda bilgi ver",
        "history": [{"text": "...", "isUser": true/false}, ...],
        "stream": false  # Streaming iÃ§in true
    }
    
    Response:
    {
        "answer": "...",
        "actions": [...],
        "model": "llama-3.3-70b",
        "tokens": 123
    }
    """
    data = request.get_json()
    question = data.get('question') or data.get('message')
    history = data.get('history', [])
    timeout = data.get('timeout', REQUEST_TIMEOUT)
    use_stream = data.get('stream', False) and STREAM_RESPONSE
    
    if not question:
        return jsonify({"error": "Soru gerekli."}), 400
    
    # Cache kontrolÃ¼
    cache_key = get_cache_key(question, history)
    cached_response = get_cached_response(cache_key)
    if cached_response:
        print(f"âœ… Cache hit: {cache_key[:8]}...")
        return jsonify(cached_response)
    
    # Streaming response
    if use_stream:
        try:
            def do_stream():
                return groq_chat(question, history, stream=True)
            
            stream = run_with_timeout(do_stream, timeout)
            if stream is None:
                return jsonify({
                    "error": "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±.",
                    "timeout": timeout
                }), 504
            
            return Response(
                stream_with_context(generate_streaming_response(stream, question)),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no',  # Nginx iÃ§in
                }
            )
        except Exception as e:
            print(f"âŒ Streaming hatasÄ±: {e}")
            # Fallback to non-streaming
            use_stream = False
    
    # Normal (non-streaming) response
    if not use_stream:
        def do_chat():
            return groq_chat(question, history, stream=False)
        
        try:
            result = run_with_timeout(do_chat, timeout)
            
            if result is None:
                return jsonify({
                    "error": "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±.",
                    "timeout": timeout
                }), 504
            
            # Aksiyon butonlarÄ± oluÅŸtur
            actions = generate_actions(question)
            
            response_data = {
                "answer": result["text"],
                "text": result["text"],  # Frontend uyumluluÄŸu iÃ§in
                "actions": actions,
                "model": result["model"],
                "tokens": result.get("tokens")
            }
            
            # Cache'e kaydet
            set_cached_response(cache_key, response_data)
            
            return jsonify(response_data)
            
        except Exception as e:
            print(f"âŒ Chat hatasÄ±: {e}")
            return jsonify({
                "error": str(e),
                "answer": "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin."
            }), 500

@app.route('/api/chat', methods=['POST'])
def api_chat():
    """Alternatif endpoint - /api/chat"""
    return chat()

# ============== LEGACY ENDPOINTS (Eski uyumluluk iÃ§in) ==============

@app.route('/predict', methods=['POST'])
def predict():
    """Zero-shot classification - Basit implementasyon"""
    data = request.get_json()
    message = data.get('message')
    candidate_labels = data.get('candidateLabels', [])
    
    if not message or not candidate_labels:
        return jsonify({"error": "Mesaj ve etiketler gerekli."}), 400
    
    # Basit keyword matching (Groq kullanmadan)
    scores = []
    message_lower = message.lower()
    
    for label in candidate_labels:
        label_lower = label.lower()
        # Basit skor hesaplama
        score = 0.1  # base score
        if label_lower in message_lower:
            score = 0.9
        elif any(word in message_lower for word in label_lower.split()):
            score = 0.6
        scores.append(score)
    
    # Normalize
    total = sum(scores)
    if total > 0:
        scores = [s/total for s in scores]
    
    return jsonify({
        "labels": candidate_labels,
        "scores": scores,
        "sequence": message
    })

# ============== BAÅLATMA ==============
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ¤– Hisse Chat AI API v2.1 (Optimized)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Model: Llama 3.3 70B (Groq API - Ãœcretsiz)                 â•‘
â•‘  Optimizasyonlar:                                            â•‘
â•‘    âœ… Streaming Response (SSE)                               â•‘
â•‘    âœ… Response Caching (5 dakika TTL)                        â•‘
â•‘    âœ… Optimize edilmiÅŸ model parametreleri                   â•‘
â•‘    âœ… Connection pooling & retry mekanizmasÄ±                  â•‘
â•‘    âœ… KÄ±saltÄ±lmÄ±ÅŸ timeout (45s)                             â•‘
â•‘  Endpoints:                                                  â•‘
â•‘    POST /chat       - AI sohbet (streaming destekli)         â•‘
â•‘    POST /api/chat   - AI sohbet (alternatif)                 â•‘
â•‘    POST /predict    - SÄ±nÄ±flandÄ±rma                         â•‘
â•‘    GET  /health     - SaÄŸlÄ±k kontrolÃ¼                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Sunucu: http://0.0.0.0:{port}                               â•‘
â•‘  Cache: {CACHE_ENABLED} | Streaming: {STREAM_RESPONSE}        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    app.run(host='0.0.0.0', port=port, threaded=True)
